# shythanya_Verbacious-Tutor
Smart Device to support Disabilities using Speech and Sign Language Conversion

TEAM MEMBERS:
	K.J. Dhaniksha Sree
	M. Dhivya                                          
	M. Krishna Shythanya                    
	G. Monisha

ABSTRACT:
     The world is becoming increasingly diverse, and technology is playing a critical role in bridging the communication gap between people who speak different languages or have different abilities. In this context, the development of a system that can convert speech to text, audio, and sign language and vice versa. The proposed system aims to provide a comprehensive solution for deaf and dumb people by incorporating additional features such as understanding all the Indian languages and converting them into English. The system will use machine learning algorithms to identify, and process spoken language in real-time, which will then be translated into text, audio, and sign language. To facilitate this process, the system will utilize a combination of cutting-edge technologies such as natural language processing, computer and image processing. The system will incorporate a webcam and a Raspberry Pi to identify gestures and convert them into text and audio in real-time. The image processing capabilities of the system will enable it to accurately identify hand gestures and convert them into corresponding signs, which will be displayed on the screen in real-time. Additionally, the system will use neural networks to recognize and process Indian languages, which will then be translated into English to enable effective communication. In conclusion, the proposed system will provide a solution for deaf and dumb people by enabling them to communicate effectively with others.

WORKING PRINCIPLE

1.Speech-to-Text Translation: The first module of our project focuses on converting speech input from the speaker into written English text. This involves using Automatic Speech Recognition (ASR) technology to analyse the speech input, break it down into individual words and sentences, and transcribe it into English text. This can be done using various translation APIs like Google Translate or Microsoft Translator. The system will use a speech recognition algorithm to convert the spoken words into text. This can be achieved using various techniques such as Hidden Markov Models (HMMs), Deep Neural Networks (DNNs), or Convolutional Neural Networks (CNNs).

 2.Text-to-Sign Translation: It focuses on translating the written English text into Indian Sign Language (ISL). This involves using Natural Language Processing (NLP) techniques to analyse the text, identify the key concepts and meaning, and generate the corresponding sign language gestures. We will use a pre-trained NLP model and fine-tune it on our specific domain to improve accuracy.
 
3.Sign Language Detection: The second module of our project focuses on detecting sign language input from the listener who is deaf and dumb. This involves using Computer Vision techniques to analyse the input from a camera, identify the hand gestures and body movements of the listener, and classify them as specific sign language gestures. We will use a pre-trained Computer Vision model and fine-tune it on our specific domain to improve accuracy.

4.Sign-to-Text Translation: It focuses on converting the detected sign language gestures into written text in English. This involves using NLP techniques to analyse the sign language gestures, identify the key concepts and meaning, and generate the corresponding English text. We will use a pre-trained NLP model and fine-tune it on our specific domain to improve accuracy. 

5.Integration and Testing: Once each module is developed, we will integrate them into a single system and perform rigorous testing to ensure that the overall system functions correctly and accurately. We will test the system on a diverse set of speech and sign language inputs, and make necessary improvements based on the results

Architecture diagram


 

                                   
RESULT AND CONCLUSION
PERFORMANCE AND ANALYSIS
To assess the performance of your project, you could consider the following metrics:
Accuracy: How accurately does your system translate speech input to Indian Sign Language (ISL) and detect sign language from the listener? You could measure accuracy by comparing the translated output or detected sign language to the actual intended message.
Speed: How quickly does your system translate speech input to ISL and detect sign language from the listener? You could measure speed by recording the time taken to translate a given input.
Robustness: How well does your system handle variations in speech and sign language? You could test your system with different accents, speech patterns, and sign language styles to assess its robustness.
User satisfaction: How satisfied are users with the translated output or detected sign language? You could collect user feedback through surveys or interviews to assess their satisfaction with the system.
To analyze the performance of your system, you could use various techniques such as:
Confusion matrix: A confusion matrix can help you visualize how often your system correctly or incorrectly classifies inputs.
Precision and recall: These metrics can help you evaluate the accuracy of your system and identify areas for improvement.
Error analysis: By analyzing the errors made by your system, you can identify patterns and common mistakes that can inform future improvements.
User feedback: Collecting user feedback can provide valuable insights into the strengths and weaknesses of your system and help guide future development.
Overall, to ensure the success of your project, it's important to continuously evaluate and analyze the performance of your system and make iterative improvements based on your findings.

